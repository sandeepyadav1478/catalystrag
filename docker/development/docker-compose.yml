services:
  qdrant:
    image: qdrant/qdrant:v1.13.5-unprivileged
    container_name: qdrant
    user: root
    deploy:
      resources:
        limits:
          cpus: '16.0'
    restart: always
    ports:
      - 6333:6333
      - 6334:6334
    expose:
      - 6333
      - 6334
      - 6335
    configs:
      - source: qdrant_config
        target: /qdrant/config/production.yaml
    volumes:
      - ./qdrant_data:/qdrant/storage


  # qdrant:
  #   build:
  #     context: ./qdrant/
  #     dockerfile: Dockerfile
  #   # image: qdrant
  #   container_name: qdrant
  #   ports:
  #     - "6333:6333"
  #   volumes:
  #     - ./qdrant:/qdrant
  #   command: ["qdrant", "start", "--config", "/qdrant/config.yml"]
  #   environment:
  #     - RUST_LOG=info
  #     - RUST_BACKTRACE=1
  #   # networks:
  #   #   - qdrant
  #   # depends_on:
  #   #   - qdrant-api
  #   #   - qdrant-indexer
  #   #   - qdrant-search

  # vllm:
  #   build:
  #     context: ./vllm/
  #     dockerfile: Dockerfile
  #   # image: vllm
  #   container_name: vllm
  #   ports:
  #     - "8080:8080"
  #   runtime: nvidia
  #   deploy: # Optional: For resourcemanagement in a Swarm or Kubernetes environment
  #     resources:
  #       reservations:
  #           devices:
  #             - driver: nvidia
  #               count: all
  #               capabilities: [gpu]
  #   command:  # Override the default entrypoint command if needed
  #     - "--model"
  #     - "qwen-7b-chat" # Or deepseek-ai/deepseek-chat-7b
  #     - "--tokenizer"
  #     - "qwen-7b-chat" # Or deepseek-ai/deepseek-chat-7b
  #     - "--quantization"
  #     - "awq"
  #     - "--gpu-memory-utilization"
  #     - "0.95"
  #     # - "--trust_remote_code" # Only for DeepSeek and other models requiring it
  #     # Add other vLLM CLI arguments as necessary
  #   # Optional: Healthcheck (Highly recommended for production)
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f http://localhost:8080/health"] # Or a more appropriate health check endpoint
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 60s
  #   restart: always # Restart the container if it crashes


  # ollama:
  #   # build:
  #   #   context: ./ollama/
  #   #   dockerfile: Dockerfile
  #   image: ollama/ollama:0.5.12
  #   container_name: ollama
  #   runtime: nvidia
  #   deploy: # Optional: For resource management in a Swarm or Kubernetes environment
  #     resources:
  #       reservations:
  #           devices:
  #             - driver: nvidia
  #               count: all
  #               capabilities: [gpu]
  #       # limits:
  #         # gpus: all # Request all available GPUs
  #       # reservations: # Optional: Request a guaranteed amount of GPU memory
  #         # gpus: all # Example: Request 1 GPU, you might need to adjust this depending on your needs.
  #   ports:
  #     - "11434:11434"
  #   entrypoint: ["/bin/sh","-c"]
  #   command: ["ollama serve"]
  #   volumes:
  #     - ./ollama-models:/root/.ollama/models
  #   tty: true
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f localhost:11434 | grep 'Ollama is running'"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 2
  #     start_period: 60s
  #   # restart: always # Restart the container if it crashes

  # deepseek14b-ollama:
  #   image: ollama/ollama:0.5.12-rocm
  #   container_name: deepseek14b-ollama
  #   runtime: nvidia
  #   deploy: # Optional: For resource management in a Swarm or Kubernetes environment
  #     resources:
  #       reservations:
  #           devices:
  #             - driver: nvidia
  #               count: all
  #               capabilities: [gpu]
  #       # limits:
  #         # gpus: all # Request all available GPUs
  #       # reservations: # Optional: Request a guaranteed amount of GPU memory
  #         # gpus: all # Example: Request 1 GPU, you might need to adjust this depending on your needs.
  #   ports:
  #     - "11435:11435"
  #   command: ["serve", "--model", "deepseek-ai/deepseek-14b", "--tokenizer", "deepseek-ai/deepseek-14b", "--quantization", "awq", "--gpu-memory-utilization", "0.95"]
  #   volumes:
  #     - ./ollama-models:/root/.ollama/models
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f http://localhost:11435 | grep 'ollama is running%'"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 2
  #     start_period: 60s
  #   restart: always # Restart the container if it crashes

  # test:
  #   image: nvidia/cuda:12.3.1-base-ubuntu20.04
  #   command: nvidia-smi
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]


# volumes:
#   ollama-models-vol: ./ollama-models/

configs:
  qdrant_config:
    content: |
      log_level: INFO